{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd and gradient descent\n",
    "\n",
    "First understand how Gradient Descent and backprop works. Its basically differential derivation wrt the specific weights and biases and changing the params in accordance with the gradient discovered in multiple steps to find some local minimum.\n",
    "\n",
    "Pt+1 = Pt - Learning rate * Gradient\n",
    "\n",
    "How we understand this process is called the Symbolic Differentiation method, which calculates each element of the gradient vector.  This is easy to understand, but hard to implement (for complex neural nets or certain activation functions) - and in practice we use other methods. Namely: \n",
    "\n",
    "#### Automatic differentiation\n",
    "\n",
    "The implementation might be different with the different frameworks, however. All frameworks use some variance of Automatic differentiation, which is conceptually difficult to understand, but easy to implement.\n",
    "\n",
    "Its hard to understand, and relies on a Taylor series expansion - a mathematical trick.  This allows fast approximation of gradients! Not strictly necessary to understand this, just know its an approximation of gradient descent which scales and is simple to implement\n",
    "\n",
    "#### Autograd Package\n",
    "\n",
    "In PyTorch the implementation comes through the AutoGrad package.  We will look at how to use this here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2,3], \n",
    "                        [4,5,6]])\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  8.,  9.],\n",
       "        [10., 11., 12.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2 = torch.Tensor([[7,8,9],\n",
    "                        [10,11,12]])\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.requires_grad # When true it tracks computations for a tensor in forward phase\n",
    "                      # and will calc gradients for this tensor in the backwards phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to enable the tracking history for the tensor\n",
    "tensor1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad)  # hasnt been used in a computation yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad_fn) # user created tensors have no grad fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.requires_grad  # cause of tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor.grad) # no gradients yet, we havent yet made any backwards passes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x7fd0e7cabc40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.grad_fn  # So it has a reference to the function that created this tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MeanBackward0 at 0x7fd0e79d6880>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Okay lets make a slightly different tensor and inspect it\n",
    "output_tensor = (tensor1 * tensor2).mean()\n",
    "output_tensor.grad_fn # It references only the last fn (note there are two funcs taking place above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.1667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1667, 1.3333, 1.5000],\n",
       "        [1.6667, 1.8333, 2.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor=tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "tensor1.requires_grad=True\n",
      "new_tensor.requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# We can use a no_grad block to force pytorch to not propagate requires grads\n",
    "with torch.no_grad():\n",
    "    new_tensor = tensor1 * 3\n",
    "    print(f'{new_tensor=}')\n",
    "    print(f'{tensor1.requires_grad=}')\n",
    "    print(f'{new_tensor.requires_grad=}') # Note that it is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(t):\n",
    "    return t * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_with_no_grad(t):\n",
    "    return t * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result_tensor=tensor([[ 2.,  4.,  6.],\\n        [ 8., 10., 12.]], grad_fn=<MulBackward0>)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor = calculate(tensor1)\n",
    "f'{result_tensor=}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result_tensor_no_grad=tensor([[ 2.,  4.,  6.],\\n        [ 8., 10., 12.]])'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor_no_grad = calculate_with_no_grad(tensor1)\n",
    "f'{result_tensor_no_grad=}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor_no_grad=tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "new_tensor_no_grad.requires_grad=False\n",
      "new_tensor=tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]], grad_fn=<MulBackward0>)\n",
      "tensor1.requires_grad=True\n",
      "new_tensor.requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# We can use a no_grad block to force pytorch to not propagate requires grads\n",
    "with torch.no_grad():\n",
    "    new_tensor_no_grad = tensor1 * 3\n",
    "    print(f'{new_tensor_no_grad=}')\n",
    "    print(f'{new_tensor_no_grad.requires_grad=}')\n",
    "    with torch.enable_grad():\n",
    "        new_tensor = tensor1 * 3\n",
    "        print(f'{new_tensor=}')\n",
    "        print(f'{tensor1.requires_grad=}')\n",
    "        print(f'{new_tensor.requires_grad=}') # Note that it is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.Tensor([2,4])\n",
    "t2 = torch.Tensor([1,2])\n",
    "t2.requires_grad_()\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5000, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = (t1 + t2).mean() - 0\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.backward()\n",
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
